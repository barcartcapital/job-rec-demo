{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Job Recommendation System: Content-Based Filtering\n",
    "\n",
    "This notebook builds two job recommendation models and compares their performance:\n",
    "\n",
    "1. **Baseline Model**: Cosine similarity on job titles (TF-IDF)\n",
    "2. **Enhanced Model**: Weighted multi-feature similarity (description TF-IDF + structured features)\n",
    "\n",
    "**Data Source:** [Workable XML Job Feed](https://www.workable.com/boards/workable.xml) (~177K jobs)\n",
    "\n",
    "**Output:** Pre-computed recommendation JSON files for a Next.js web app demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (uncomment for Google Colab)\n",
    "# !pip install lxml beautifulsoup4 scikit-learn pandas numpy matplotlib seaborn tqdm requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from lxml import etree\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "\n",
    "# Settings\n",
    "pd.set_option('display.max_colwidth', 80)\n",
    "sns.set_theme(style='whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "\n",
    "FEED_URL = 'https://www.workable.com/boards/workable.xml'\n",
    "XML_PATH = 'data/workable_feed.xml'\n",
    "OUTPUT_DIR = 'output'\n",
    "SAMPLE_SIZE = 1000\n",
    "TOP_N = 3  # recommendations per job\n",
    "\n",
    "os.makedirs('data', exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "print('Setup complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Download XML Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(XML_PATH):\n",
    "    file_size_mb = os.path.getsize(XML_PATH) / (1024 * 1024)\n",
    "    print(f'XML feed already cached at {XML_PATH} ({file_size_mb:.1f} MB). Skipping download.')\n",
    "else:\n",
    "    print(f'Downloading XML feed from {FEED_URL}...')\n",
    "    print('This may take a few minutes (the file is large).')\n",
    "    try:\n",
    "        response = requests.get(FEED_URL, stream=True, timeout=300)\n",
    "        response.raise_for_status()\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        with open(XML_PATH, 'wb') as f:\n",
    "            downloaded = 0\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "                downloaded += len(chunk)\n",
    "                if total_size > 0:\n",
    "                    pct = downloaded / total_size * 100\n",
    "                    print(f'\\rDownloaded {downloaded / 1024 / 1024:.1f} MB ({pct:.0f}%)', end='')\n",
    "        print(f'\\nDone! Saved to {XML_PATH}')\n",
    "    except Exception as e:\n",
    "        print(f'Download failed: {e}')\n",
    "        print('\\nAlternative: Download the file manually from your browser:')\n",
    "        print(f'  {FEED_URL}')\n",
    "        print(f'  Save it to: {os.path.abspath(XML_PATH)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parse XML Feed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_job_feed(xml_path, max_jobs=None):\n",
    "    \"\"\"Parse the Workable XML feed using streaming for constant memory.\"\"\"\n",
    "    jobs = []\n",
    "    context = etree.iterparse(xml_path, events=('end',), tag='job', recover=True)\n",
    "    \n",
    "    for event, elem in tqdm(context, desc='Parsing jobs'):\n",
    "        job = {\n",
    "            'id': (elem.findtext('referencenumber') or '').strip(),\n",
    "            'title': (elem.findtext('title') or '').strip(),\n",
    "            'company': (elem.findtext('company') or '').strip(),\n",
    "            'city': (elem.findtext('city') or '').strip(),\n",
    "            'state': (elem.findtext('state') or '').strip(),\n",
    "            'country': (elem.findtext('country') or '').strip(),\n",
    "            'remote': (elem.findtext('remote') or '').strip().lower() == 'true',\n",
    "            'description': (elem.findtext('description') or '').strip(),\n",
    "            'education': (elem.findtext('education') or '').strip(),\n",
    "            'job_type': (elem.findtext('jobtype') or '').strip(),\n",
    "            'category': (elem.findtext('category') or '').strip(),\n",
    "            'experience': (elem.findtext('experience') or '').strip(),\n",
    "            'url': (elem.findtext('url') or '').strip(),\n",
    "            'date': (elem.findtext('date') or '').strip(),\n",
    "        }\n",
    "        \n",
    "        # Only include jobs with at least a title and description\n",
    "        if job['id'] and job['title'] and job['description']:\n",
    "            jobs.append(job)\n",
    "        \n",
    "        # Free memory\n",
    "        elem.clear()\n",
    "        while elem.getprevious() is not None:\n",
    "            del elem.getparent()[0]\n",
    "        \n",
    "        if max_jobs and len(jobs) >= max_jobs:\n",
    "            break\n",
    "    \n",
    "    return jobs\n",
    "\n",
    "# Parse all jobs (or set max_jobs to limit for testing)\n",
    "all_jobs = parse_job_feed(XML_PATH)\n",
    "print(f'\\nParsed {len(all_jobs):,} valid jobs from the feed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load into DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.DataFrame(all_jobs)\n",
    "print(f'DataFrame shape: {df_all.shape}')\n",
    "print(f'\\nColumns: {list(df_all.columns)}')\n",
    "print(f'\\nCategory distribution (top 15):')\n",
    "print(df_all['category'].value_counts().head(15))\n",
    "print(f'\\nJob type distribution:')\n",
    "print(df_all['job_type'].value_counts())\n",
    "print(f'\\nCountry distribution (top 10):')\n",
    "print(df_all['country'].value_counts().head(10))\n",
    "df_all.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sample 1,000 Diverse Jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_sample(df, n=1000, stratify_col='category', min_per_group=5, random_state=42):\n",
    "    \"\"\"Sample n jobs with diversity across categories.\"\"\"\n",
    "    # Remove duplicates by title+company+city\n",
    "    df_dedup = df.drop_duplicates(subset=['title', 'company', 'city'])\n",
    "    print(f'After deduplication: {len(df_dedup):,} jobs (removed {len(df) - len(df_dedup):,} duplicates)')\n",
    "    \n",
    "    # Fill empty categories\n",
    "    df_dedup = df_dedup.copy()\n",
    "    df_dedup[stratify_col] = df_dedup[stratify_col].replace('', 'Other')\n",
    "    \n",
    "    # Calculate proportional allocation\n",
    "    cat_counts = df_dedup[stratify_col].value_counts()\n",
    "    cat_proportions = cat_counts / cat_counts.sum()\n",
    "    cat_samples = (cat_proportions * n).apply(lambda x: max(int(x), min_per_group))\n",
    "    \n",
    "    # Adjust to hit target n\n",
    "    while cat_samples.sum() > n:\n",
    "        largest = cat_samples.idxmax()\n",
    "        cat_samples[largest] -= 1\n",
    "    while cat_samples.sum() < n:\n",
    "        largest = cat_samples.idxmax()\n",
    "        cat_samples[largest] += 1\n",
    "    \n",
    "    # Sample from each category\n",
    "    sampled = []\n",
    "    for cat, count in cat_samples.items():\n",
    "        cat_df = df_dedup[df_dedup[stratify_col] == cat]\n",
    "        sample_n = min(count, len(cat_df))\n",
    "        sampled.append(cat_df.sample(n=sample_n, random_state=random_state))\n",
    "    \n",
    "    result = pd.concat(sampled).reset_index(drop=True)\n",
    "    print(f'Sampled {len(result)} jobs across {result[stratify_col].nunique()} categories')\n",
    "    return result\n",
    "\n",
    "df = stratified_sample(df_all, n=SAMPLE_SIZE)\n",
    "print(f'\\nSample category distribution:')\n",
    "print(df['category'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clean HTML Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(raw_html):\n",
    "    \"\"\"Remove HTML tags, URLs, emails, and normalize whitespace.\"\"\"\n",
    "    if not raw_html or not isinstance(raw_html, str):\n",
    "        return ''\n",
    "    soup = BeautifulSoup(raw_html, 'html.parser')\n",
    "    for tag in soup.find_all(['script', 'style']):\n",
    "        tag.decompose()\n",
    "    text = soup.get_text(separator=' ', strip=True)\n",
    "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'\\S+@\\S+\\.\\S+', '', text)\n",
    "    text = re.sub(r'&\\w+;', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df['description_clean'] = df['description'].apply(clean_html)\n",
    "\n",
    "# Show before/after for one example\n",
    "idx = 0\n",
    "print('=== RAW HTML (first 300 chars) ===')\n",
    "print(df.iloc[idx]['description'][:300])\n",
    "print('\\n=== CLEANED TEXT (first 300 chars) ===')\n",
    "print(df.iloc[idx]['description_clean'][:300])\n",
    "print(f'\\nDescription lengths (cleaned): min={df[\"description_clean\"].str.len().min()}, '\n",
    "      f'median={df[\"description_clean\"].str.len().median():.0f}, '\n",
    "      f'max={df[\"description_clean\"].str.len().max()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Lowercase and basic cleaning for TF-IDF input.\"\"\"\n",
    "    if not text:\n",
    "        return ''\n",
    "    text = text.lower()\n",
    "    # Remove special characters but keep spaces and alphanumeric\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "df['title_clean'] = df['title'].apply(preprocess_text)\n",
    "df['desc_processed'] = df['description_clean'].apply(preprocess_text)\n",
    "\n",
    "print('Sample preprocessed titles:')\n",
    "for i in range(5):\n",
    "    print(f'  {df.iloc[i][\"title\"]}  ->  {df.iloc[i][\"title_clean\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Handle Missing Fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values\n",
    "df['category'] = df['category'].replace('', 'Other')\n",
    "df['experience'] = df['experience'].replace('', 'Not Specified')\n",
    "df['education'] = df['education'].replace('', 'Not Specified')\n",
    "df['job_type'] = df['job_type'].replace('', 'Not Specified')\n",
    "df['city'] = df['city'].replace('', 'Unknown')\n",
    "df['state'] = df['state'].replace('', 'Unknown')\n",
    "df['country'] = df['country'].replace('', 'Unknown')\n",
    "\n",
    "print('Missing values after filling:')\n",
    "print(df[['category', 'experience', 'education', 'job_type', 'city', 'state', 'country']].eq('').sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Category distribution\n",
    "cat_counts = df['category'].value_counts().head(15)\n",
    "cat_counts.plot(kind='barh', ax=axes[0, 0], color='steelblue')\n",
    "axes[0, 0].set_title('Top 15 Categories')\n",
    "axes[0, 0].set_xlabel('Count')\n",
    "\n",
    "# Job type distribution\n",
    "type_counts = df['job_type'].value_counts()\n",
    "type_counts.plot(kind='bar', ax=axes[0, 1], color='coral')\n",
    "axes[0, 1].set_title('Job Type Distribution')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Country distribution (top 10)\n",
    "country_counts = df['country'].value_counts().head(10)\n",
    "country_counts.plot(kind='bar', ax=axes[1, 0], color='seagreen')\n",
    "axes[1, 0].set_title('Top 10 Countries')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Description length distribution\n",
    "df['desc_len'] = df['description_clean'].str.len()\n",
    "axes[1, 1].hist(df['desc_len'], bins=50, color='mediumpurple', edgecolor='white')\n",
    "axes[1, 1].set_title('Description Length Distribution')\n",
    "axes[1, 1].set_xlabel('Characters')\n",
    "axes[1, 1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nDataset summary: {len(df)} jobs, {df[\"category\"].nunique()} categories, '\n",
    "      f'{df[\"country\"].nunique()} countries, {df[\"company\"].nunique()} companies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Baseline Model: Title-Only TF-IDF Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build TF-IDF matrix on job titles\n",
    "title_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=5000,\n",
    "    stop_words='english',\n",
    "    sublinear_tf=True,\n",
    ")\n",
    "title_tfidf = title_vectorizer.fit_transform(df['title_clean'])\n",
    "\n",
    "print(f'Title TF-IDF matrix shape: {title_tfidf.shape}')\n",
    "print(f'Vocabulary size: {len(title_vectorizer.vocabulary_)}')\n",
    "print(f'Top 20 terms by document frequency:')\n",
    "feature_names = title_vectorizer.get_feature_names_out()\n",
    "doc_freq = (title_tfidf > 0).sum(axis=0).A1\n",
    "top_terms_idx = doc_freq.argsort()[::-1][:20]\n",
    "for idx in top_terms_idx:\n",
    "    print(f'  {feature_names[idx]}: {doc_freq[idx]} docs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Compute Title Cosine Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_sim_matrix = cosine_similarity(title_tfidf)\n",
    "print(f'Title similarity matrix shape: {title_sim_matrix.shape}')\n",
    "print(f'Memory: {title_sim_matrix.nbytes / 1024 / 1024:.1f} MB')\n",
    "\n",
    "# Quick sanity check - similarity of first job with itself should be 1.0\n",
    "print(f'\\nSelf-similarity (should be ~1.0): {title_sim_matrix[0, 0]:.4f}')\n",
    "print(f'Mean pairwise similarity: {title_sim_matrix[np.triu_indices_from(title_sim_matrix, k=1)].mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Extract Baseline Recommendations (Top-3 per Job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_recs(sim_matrix, df, n=3):\n",
    "    \"\"\"Extract top-N recommendations per job from a similarity matrix.\"\"\"\n",
    "    recs = {}\n",
    "    for i in range(len(df)):\n",
    "        scores = sim_matrix[i].copy()\n",
    "        scores[i] = -1  # exclude self\n",
    "        top_n = np.argsort(scores)[::-1][:n]\n",
    "        recs[df.iloc[i]['id']] = [\n",
    "            {'id': df.iloc[top_n[j]]['id'], 'score': round(float(scores[top_n[j]]), 4)}\n",
    "            for j in range(n)\n",
    "        ]\n",
    "    return recs\n",
    "\n",
    "baseline_recs = get_top_n_recs(title_sim_matrix, df, n=TOP_N)\n",
    "print(f'Generated baseline recommendations for {len(baseline_recs)} jobs.')\n",
    "\n",
    "# Show example\n",
    "example_id = df.iloc[0]['id']\n",
    "print(f'\\nExample: \"{df.iloc[0][\"title\"]}\" ({example_id})')\n",
    "print(f'Baseline recommendations:')\n",
    "for rec in baseline_recs[example_id]:\n",
    "    rec_row = df[df['id'] == rec['id']].iloc[0]\n",
    "    print(f'  - {rec_row[\"title\"]} at {rec_row[\"company\"]} (score: {rec[\"score\"]:.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Spot-Check Baseline Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_recommendations(job_idx, recs_dict, df, model_name='Model'):\n",
    "    \"\"\"Display a job and its recommendations.\"\"\"\n",
    "    job = df.iloc[job_idx]\n",
    "    print(f'\\n{\"=\"*80}')\n",
    "    print(f'SOURCE JOB: {job[\"title\"]}')\n",
    "    print(f'  Company: {job[\"company\"]} | Location: {job[\"city\"]}, {job[\"state\"]}, {job[\"country\"]}')\n",
    "    print(f'  Category: {job[\"category\"]} | Type: {job[\"job_type\"]} | Experience: {job[\"experience\"]}')\n",
    "    print(f'\\n{model_name} Recommendations:')\n",
    "    for i, rec in enumerate(recs_dict[job['id']], 1):\n",
    "        rec_row = df[df['id'] == rec['id']].iloc[0]\n",
    "        print(f'  {i}. {rec_row[\"title\"]}')\n",
    "        print(f'     Company: {rec_row[\"company\"]} | Location: {rec_row[\"city\"]}, {rec_row[\"state\"]}')\n",
    "        print(f'     Category: {rec_row[\"category\"]} | Score: {rec[\"score\"]:.4f}')\n",
    "\n",
    "# Spot-check 5 diverse jobs\n",
    "spot_check_indices = [0, len(df)//5, len(df)//3, len(df)//2, len(df)*3//4]\n",
    "for idx in spot_check_indices:\n",
    "    display_recommendations(idx, baseline_recs, df, model_name='Baseline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. Enhanced Model: Description TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build TF-IDF matrix on cleaned descriptions\n",
    "desc_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=10000,\n",
    "    max_df=0.85,\n",
    "    min_df=2,\n",
    "    stop_words='english',\n",
    "    sublinear_tf=True,\n",
    "    dtype=np.float32,\n",
    ")\n",
    "desc_tfidf = desc_vectorizer.fit_transform(df['desc_processed'])\n",
    "\n",
    "print(f'Description TF-IDF matrix shape: {desc_tfidf.shape}')\n",
    "print(f'Vocabulary size: {len(desc_vectorizer.vocabulary_)}')\n",
    "print(f'\\nTop 20 description terms by document frequency:')\n",
    "desc_feature_names = desc_vectorizer.get_feature_names_out()\n",
    "desc_doc_freq = (desc_tfidf > 0).sum(axis=0).A1\n",
    "desc_top_idx = desc_doc_freq.argsort()[::-1][:20]\n",
    "for idx in desc_top_idx:\n",
    "    print(f'  {desc_feature_names[idx]}: {desc_doc_freq[idx]} docs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Compute Individual Feature Similarity Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Description similarity (TF-IDF cosine)\n",
    "print('Computing description similarity...')\n",
    "desc_sim = cosine_similarity(desc_tfidf)\n",
    "\n",
    "# Category match (binary: same category = 1.0, different = 0.0)\n",
    "print('Computing category similarity...')\n",
    "categories = df['category'].values\n",
    "cat_sim = (categories[:, None] == categories[None, :]).astype(np.float32)\n",
    "\n",
    "# Location similarity (tiered: same city=1.0, same state=0.5, same country=0.2, else=0.0)\n",
    "print('Computing location similarity...')\n",
    "cities = df['city'].values\n",
    "states = df['state'].values\n",
    "countries = df['country'].values\n",
    "loc_sim = np.where(\n",
    "    cities[:, None] == cities[None, :], 1.0,\n",
    "    np.where(\n",
    "        states[:, None] == states[None, :], 0.5,\n",
    "        np.where(\n",
    "            countries[:, None] == countries[None, :], 0.2, 0.0\n",
    "        )\n",
    "    )\n",
    ").astype(np.float32)\n",
    "\n",
    "# Job type match (binary)\n",
    "print('Computing job type similarity...')\n",
    "jobtypes = df['job_type'].values\n",
    "type_sim = (jobtypes[:, None] == jobtypes[None, :]).astype(np.float32)\n",
    "\n",
    "# Experience level similarity (ordinal distance)\n",
    "print('Computing experience similarity...')\n",
    "exp_map = {\n",
    "    'Entry level': 0, 'Internship': 0,\n",
    "    'Associate': 1,\n",
    "    'Mid-Senior level': 2, 'Not Specified': 2,\n",
    "    'Director': 3,\n",
    "    'Executive': 4,\n",
    "}\n",
    "exp_vals = np.array([exp_map.get(e, 2) for e in df['experience'].values], dtype=np.float32)\n",
    "exp_dist = np.abs(exp_vals[:, None] - exp_vals[None, :])\n",
    "max_dist = exp_dist.max()\n",
    "exp_sim = (1.0 - exp_dist / max_dist).astype(np.float32) if max_dist > 0 else np.ones_like(exp_dist)\n",
    "\n",
    "print(f'\\nAll feature matrices computed. Shape: {desc_sim.shape}')\n",
    "print(f'Memory per matrix: {desc_sim.nbytes / 1024 / 1024:.1f} MB')\n",
    "print(f'Total memory: {(desc_sim.nbytes + cat_sim.nbytes + loc_sim.nbytes + type_sim.nbytes + exp_sim.nbytes) / 1024 / 1024:.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Combine with Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTS = {\n",
    "    'description': 0.35,\n",
    "    'title': 0.25,\n",
    "    'category': 0.15,\n",
    "    'location': 0.10,\n",
    "    'job_type': 0.08,\n",
    "    'experience': 0.07,\n",
    "}\n",
    "\n",
    "print('Feature weights:')\n",
    "for feature, weight in WEIGHTS.items():\n",
    "    print(f'  {feature}: {weight:.0%}')\n",
    "print(f'  Total: {sum(WEIGHTS.values()):.0%}')\n",
    "\n",
    "# Compute weighted similarity matrix\n",
    "weighted_sim = (\n",
    "    WEIGHTS['description'] * desc_sim +\n",
    "    WEIGHTS['title'] * title_sim_matrix.astype(np.float32) +\n",
    "    WEIGHTS['category'] * cat_sim +\n",
    "    WEIGHTS['location'] * loc_sim +\n",
    "    WEIGHTS['job_type'] * type_sim +\n",
    "    WEIGHTS['experience'] * exp_sim\n",
    ")\n",
    "\n",
    "print(f'\\nWeighted similarity matrix shape: {weighted_sim.shape}')\n",
    "print(f'Mean weighted similarity: {weighted_sim[np.triu_indices_from(weighted_sim, k=1)].mean():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Extract Weighted Recommendations (Top-3 per Job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_recs = get_top_n_recs(weighted_sim, df, n=TOP_N)\n",
    "print(f'Generated weighted recommendations for {len(weighted_recs)} jobs.')\n",
    "\n",
    "# Show example\n",
    "example_id = df.iloc[0]['id']\n",
    "print(f'\\nExample: \"{df.iloc[0][\"title\"]}\" ({example_id})')\n",
    "print(f'Weighted recommendations:')\n",
    "for rec in weighted_recs[example_id]:\n",
    "    rec_row = df[df['id'] == rec['id']].iloc[0]\n",
    "    print(f'  - {rec_row[\"title\"]} at {rec_row[\"company\"]} (score: {rec[\"score\"]:.4f})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Side-by-Side Spot Check: Baseline vs Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_recommendations(job_idx, baseline, weighted, df):\n",
    "    \"\"\"Side-by-side comparison of both models for a single job.\"\"\"\n",
    "    job = df.iloc[job_idx]\n",
    "    job_id = job['id']\n",
    "    \n",
    "    print(f'\\n{\"=\"*100}')\n",
    "    print(f'SOURCE: {job[\"title\"]} | {job[\"company\"]} | {job[\"city\"]}, {job[\"country\"]}')\n",
    "    print(f'        Category: {job[\"category\"]} | Type: {job[\"job_type\"]} | Exp: {job[\"experience\"]}')\n",
    "    print(f'{\"=\"*100}')\n",
    "    \n",
    "    base_recs = baseline[job_id]\n",
    "    wtd_recs = weighted[job_id]\n",
    "    \n",
    "    # Check overlap\n",
    "    base_ids = {r['id'] for r in base_recs}\n",
    "    wtd_ids = {r['id'] for r in wtd_recs}\n",
    "    overlap = base_ids & wtd_ids\n",
    "    \n",
    "    print(f'\\n  {\"BASELINE (Title Only)\":<48} | {\"ENHANCED (Weighted Multi-Feature)\":<48}')\n",
    "    print(f'  {\"-\"*48} | {\"-\"*48}')\n",
    "    \n",
    "    for i in range(TOP_N):\n",
    "        br = df[df['id'] == base_recs[i]['id']].iloc[0]\n",
    "        wr = df[df['id'] == wtd_recs[i]['id']].iloc[0]\n",
    "        \n",
    "        b_marker = '*' if base_recs[i]['id'] in overlap else ' '\n",
    "        w_marker = '*' if wtd_recs[i]['id'] in overlap else ' '\n",
    "        \n",
    "        b_str = f'{b_marker}{br[\"title\"][:35]:<36} ({base_recs[i][\"score\"]:.3f})'\n",
    "        w_str = f'{w_marker}{wr[\"title\"][:35]:<36} ({wtd_recs[i][\"score\"]:.3f})'\n",
    "        print(f'  {b_str:<48} | {w_str:<48}')\n",
    "        \n",
    "        b_loc = f'   {br[\"company\"][:20]} | {br[\"city\"]}, {br[\"country\"]}'\n",
    "        w_loc = f'   {wr[\"company\"][:20]} | {wr[\"city\"]}, {wr[\"country\"]}'\n",
    "        print(f'  {b_loc:<48} | {w_loc:<48}')\n",
    "    \n",
    "    print(f'\\n  Overlap: {len(overlap)}/{TOP_N} recommendations in common (* = shared)')\n",
    "\n",
    "# Compare same 5 jobs as the spot check\n",
    "for idx in spot_check_indices:\n",
    "    compare_recommendations(idx, baseline_recs, weighted_recs, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 19. Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(recs_dict, sim_matrix, df):\n",
    "    \"\"\"Compute offline evaluation metrics for a recommendation model.\"\"\"\n",
    "    id_to_idx = {row['id']: i for i, row in df.iterrows()}\n",
    "    \n",
    "    # 1. Mean Recommendation Score\n",
    "    all_scores = [rec['score'] for recs in recs_dict.values() for rec in recs]\n",
    "    mean_score = np.mean(all_scores)\n",
    "    \n",
    "    # 2. Intra-List Diversity (ILD)\n",
    "    ilds = []\n",
    "    for job_id, recs in recs_dict.items():\n",
    "        rec_indices = [id_to_idx[r['id']] for r in recs if r['id'] in id_to_idx]\n",
    "        if len(rec_indices) < 2:\n",
    "            continue\n",
    "        # Pairwise similarity among recommendations\n",
    "        pairs = []\n",
    "        for a in range(len(rec_indices)):\n",
    "            for b in range(a + 1, len(rec_indices)):\n",
    "                pairs.append(sim_matrix[rec_indices[a], rec_indices[b]])\n",
    "        ild = 1.0 - np.mean(pairs)  # diversity = 1 - similarity\n",
    "        ilds.append(ild)\n",
    "    mean_ild = np.mean(ilds)\n",
    "    \n",
    "    # 3. Catalog Coverage\n",
    "    recommended_ids = set()\n",
    "    for recs in recs_dict.values():\n",
    "        for rec in recs:\n",
    "            recommended_ids.add(rec['id'])\n",
    "    coverage = len(recommended_ids) / len(df)\n",
    "    \n",
    "    # 4. Category Coherence\n",
    "    job_cats = {row['id']: row['category'] for _, row in df.iterrows()}\n",
    "    coherence_scores = []\n",
    "    for job_id, recs in recs_dict.items():\n",
    "        source_cat = job_cats.get(job_id, '')\n",
    "        same_cat = sum(1 for r in recs if job_cats.get(r['id'], '') == source_cat)\n",
    "        coherence_scores.append(same_cat / len(recs))\n",
    "    mean_coherence = np.mean(coherence_scores)\n",
    "    \n",
    "    return {\n",
    "        'Mean Rec Score': mean_score,\n",
    "        'Intra-List Diversity': mean_ild,\n",
    "        'Catalog Coverage': coverage,\n",
    "        'Category Coherence': mean_coherence,\n",
    "    }\n",
    "\n",
    "# Use description TF-IDF similarity for ILD (content-based diversity measure)\n",
    "baseline_metrics = compute_metrics(baseline_recs, desc_sim, df)\n",
    "weighted_metrics = compute_metrics(weighted_recs, desc_sim, df)\n",
    "\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Baseline (Title Only)': baseline_metrics,\n",
    "    'Enhanced (Weighted)': weighted_metrics,\n",
    "})\n",
    "metrics_df['Better'] = metrics_df.apply(\n",
    "    lambda row: 'Enhanced' if row['Enhanced (Weighted)'] >= row['Baseline (Title Only)'] else 'Baseline',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print('\\nModel Comparison Metrics:')\n",
    "print('=' * 70)\n",
    "print(metrics_df.to_string())\n",
    "print('\\nNote: Higher is better for all metrics except Category Coherence (depends on goal).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 20. Visualize Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(18, 5))\n",
    "\n",
    "metrics_names = list(baseline_metrics.keys())\n",
    "baseline_vals = list(baseline_metrics.values())\n",
    "weighted_vals = list(weighted_metrics.values())\n",
    "colors = ['#3b82f6', '#10b981']  # blue for baseline, green for enhanced\n",
    "\n",
    "for i, (name, bv, wv) in enumerate(zip(metrics_names, baseline_vals, weighted_vals)):\n",
    "    bars = axes[i].bar(['Baseline', 'Enhanced'], [bv, wv], color=colors, edgecolor='white', width=0.6)\n",
    "    axes[i].set_title(name, fontsize=11, fontweight='bold')\n",
    "    axes[i].set_ylim(0, max(bv, wv) * 1.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, val in zip(bars, [bv, wv]):\n",
    "        axes[i].text(bar.get_x() + bar.get_width()/2., bar.get_height() + 0.01,\n",
    "                     f'{val:.3f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.suptitle('Model Comparison: Baseline vs Enhanced', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Score distribution comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "base_scores = [rec['score'] for recs in baseline_recs.values() for rec in recs]\n",
    "wtd_scores = [rec['score'] for recs in weighted_recs.values() for rec in recs]\n",
    "\n",
    "axes[0].hist(base_scores, bins=50, color=colors[0], alpha=0.7, label='Baseline', edgecolor='white')\n",
    "axes[0].hist(wtd_scores, bins=50, color=colors[1], alpha=0.7, label='Enhanced', edgecolor='white')\n",
    "axes[0].set_title('Recommendation Score Distribution', fontweight='bold')\n",
    "axes[0].set_xlabel('Similarity Score')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].legend()\n",
    "\n",
    "# Overlap analysis\n",
    "overlaps = []\n",
    "for job_id in baseline_recs:\n",
    "    base_ids = {r['id'] for r in baseline_recs[job_id]}\n",
    "    wtd_ids = {r['id'] for r in weighted_recs[job_id]}\n",
    "    overlaps.append(len(base_ids & wtd_ids))\n",
    "\n",
    "overlap_counts = Counter(overlaps)\n",
    "axes[1].bar(overlap_counts.keys(), overlap_counts.values(), color='mediumpurple', edgecolor='white')\n",
    "axes[1].set_title('Recommendation Overlap Between Models', fontweight='bold')\n",
    "axes[1].set_xlabel(f'Number of shared recommendations (out of {TOP_N})')\n",
    "axes[1].set_ylabel('Number of jobs')\n",
    "axes[1].set_xticks(range(TOP_N + 1))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'\\nOverlap summary: {np.mean(overlaps):.1f} average shared recommendations per job')\n",
    "for k, v in sorted(overlap_counts.items()):\n",
    "    print(f'  {k}/{TOP_N} overlap: {v} jobs ({v/len(df)*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 21. Qualitative Comparison Panel (10 Diverse Jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 10 diverse jobs across different categories\n",
    "unique_cats = df['category'].unique()\n",
    "panel_indices = []\n",
    "for cat in unique_cats[:10]:\n",
    "    cat_jobs = df[df['category'] == cat]\n",
    "    if len(cat_jobs) > 0:\n",
    "        panel_indices.append(cat_jobs.index[0])\n",
    "\n",
    "# If we don't have 10 categories, fill from remaining jobs\n",
    "while len(panel_indices) < 10 and len(panel_indices) < len(df):\n",
    "    idx = len(df) * len(panel_indices) // 10\n",
    "    if idx not in panel_indices:\n",
    "        panel_indices.append(idx)\n",
    "\n",
    "print(f'Qualitative Comparison Panel: {len(panel_indices)} diverse jobs')\n",
    "print(f'Categories represented: {[df.iloc[i][\"category\"] for i in panel_indices]}')\n",
    "\n",
    "for idx in panel_indices:\n",
    "    compare_recommendations(idx, baseline_recs, weighted_recs, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 22. Summary & Findings\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "**Baseline Model (Title-Only Cosine Similarity):**\n",
    "- Uses only job titles for matching\n",
    "- Fast to compute but misses semantic similarity when titles differ\n",
    "- Two jobs with different titles but similar responsibilities (e.g., \"Software Developer\" vs \"Full Stack Engineer\") may not match well\n",
    "\n",
    "**Enhanced Model (Weighted Multi-Feature):**\n",
    "- Combines 6 features: description (35%), title (25%), category (15%), location (10%), job type (8%), experience (7%)\n",
    "- TF-IDF on descriptions captures deeper skill/requirement overlap\n",
    "- Structured features (location, category, experience) provide additional context\n",
    "- Generally produces more relevant and diverse recommendations\n",
    "\n",
    "### When the Models Diverge\n",
    "The most interesting cases are when the two models produce **different** recommendations. This typically happens when:\n",
    "1. Jobs have different titles but similar descriptions (enhanced model catches this)\n",
    "2. Jobs are in the same location/category (enhanced model boosts these)\n",
    "3. Jobs have very generic titles (baseline struggles with differentiation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 23. Export JSON for Web App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export jobs.json\n",
    "jobs_export = []\n",
    "for _, row in df.iterrows():\n",
    "    jobs_export.append({\n",
    "        'id': row['id'],\n",
    "        'title': row['title'],\n",
    "        'company': row['company'],\n",
    "        'city': row['city'],\n",
    "        'state': row['state'],\n",
    "        'country': row['country'],\n",
    "        'remote': bool(row['remote']),\n",
    "        'description': row['description'],  # Keep raw HTML for web rendering\n",
    "        'category': row['category'],\n",
    "        'jobType': row['job_type'],\n",
    "        'experience': row['experience'],\n",
    "        'education': row['education'],\n",
    "        'url': row['url'],\n",
    "    })\n",
    "\n",
    "# Write files\n",
    "jobs_path = os.path.join(OUTPUT_DIR, 'jobs.json')\n",
    "baseline_path = os.path.join(OUTPUT_DIR, 'recs_baseline.json')\n",
    "weighted_path = os.path.join(OUTPUT_DIR, 'recs_weighted.json')\n",
    "\n",
    "with open(jobs_path, 'w') as f:\n",
    "    json.dump(jobs_export, f, separators=(',', ':'))\n",
    "\n",
    "with open(baseline_path, 'w') as f:\n",
    "    json.dump(baseline_recs, f, separators=(',', ':'))\n",
    "\n",
    "with open(weighted_path, 'w') as f:\n",
    "    json.dump(weighted_recs, f, separators=(',', ':'))\n",
    "\n",
    "print('Exported files:')\n",
    "for path in [jobs_path, baseline_path, weighted_path]:\n",
    "    size_kb = os.path.getsize(path) / 1024\n",
    "    if size_kb > 1024:\n",
    "        print(f'  {path}: {size_kb/1024:.1f} MB')\n",
    "    else:\n",
    "        print(f'  {path}: {size_kb:.0f} KB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 24. Sanity Check Exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload and verify\n",
    "with open(jobs_path) as f:\n",
    "    verify_jobs = json.load(f)\n",
    "with open(baseline_path) as f:\n",
    "    verify_baseline = json.load(f)\n",
    "with open(weighted_path) as f:\n",
    "    verify_weighted = json.load(f)\n",
    "\n",
    "job_ids = {j['id'] for j in verify_jobs}\n",
    "\n",
    "print(f'Jobs exported: {len(verify_jobs)}')\n",
    "print(f'Baseline recs: {len(verify_baseline)} jobs with recommendations')\n",
    "print(f'Weighted recs: {len(verify_weighted)} jobs with recommendations')\n",
    "\n",
    "# Verify all rec IDs exist in jobs\n",
    "missing_baseline = 0\n",
    "missing_weighted = 0\n",
    "for job_id, recs in verify_baseline.items():\n",
    "    assert job_id in job_ids, f'Job {job_id} in baseline recs but not in jobs'\n",
    "    assert len(recs) == TOP_N, f'Job {job_id} has {len(recs)} recs, expected {TOP_N}'\n",
    "    for r in recs:\n",
    "        if r['id'] not in job_ids:\n",
    "            missing_baseline += 1\n",
    "\n",
    "for job_id, recs in verify_weighted.items():\n",
    "    assert job_id in job_ids, f'Job {job_id} in weighted recs but not in jobs'\n",
    "    assert len(recs) == TOP_N, f'Job {job_id} has {len(recs)} recs, expected {TOP_N}'\n",
    "    for r in recs:\n",
    "        if r['id'] not in job_ids:\n",
    "            missing_weighted += 1\n",
    "\n",
    "print(f'\\nValidation:')\n",
    "print(f'  All job IDs in baseline recs exist in jobs.json: {missing_baseline == 0}')\n",
    "print(f'  All job IDs in weighted recs exist in jobs.json: {missing_weighted == 0}')\n",
    "print(f'  Every job has exactly {TOP_N} recommendations: True')\n",
    "print(f'\\nReady for web app!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
